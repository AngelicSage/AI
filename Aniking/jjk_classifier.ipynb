{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "base_path = '/Users/whybless/Documents/ai/Aniking'\n",
    "\n",
    "backup_data = os.path.join(base_path, 'Backup data')\n",
    "backup_train_data = os.path.join(backup_data, 'train')\n",
    "backup_val_data = os.path.join(backup_data, 'val')\n",
    "TRAINING_DIR = os.path.join(base_path, 'clean_jjk_data/train')\n",
    "VALIDATION_DIR = os.path.join(base_path, 'clean_jjk_data/val')\n",
    "\n",
    "def copy_and_reset_data(source_dir, destination_dir):\n",
    "    \"\"\"Copies data from the source directory to the destination directory,\n",
    "       deleting the destination directory (if it exists) before copying.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Path to the source directory.\n",
    "        destination_dir (str): Path to the destination directory.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(destination_dir):\n",
    "            shutil.rmtree(destination_dir)\n",
    "        shutil.copytree(source_dir, destination_dir)\n",
    "    except OSError as e:\n",
    "        print(f\"Error copying data: {e}\")\n",
    "\n",
    "\n",
    "# Copy train data\n",
    "copy_and_reset_data(backup_train_data, TRAINING_DIR)\n",
    "\n",
    "# Copy validation data (using backup_val_data)\n",
    "copy_and_reset_data(backup_val_data, VALIDATION_DIR)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For backup\n",
      "Number of training dirs is 22\n",
      "Number of validation dirs is 22\n",
      "For data\n",
      "Number of training dirs is 22\n",
      "Number of validation dirs is 22\n"
     ]
    }
   ],
   "source": [
    "print('For backup')\n",
    "print(f\"Number of training dirs is {len(os.listdir(backup_train_data))}\")\n",
    "print(f\"Number of validation dirs is {len(os.listdir(backup_val_data))}\")\n",
    "print('For data')\n",
    "print(f\"Number of training dirs is {len(os.listdir(TRAINING_DIR))}\")\n",
    "print(f\"Number of validation dirs is {len(os.listdir(VALIDATION_DIR))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Itadori backup training files is 760\n",
      "Number of Itadori training  files is 760\n",
      "['Kento Nanami', 'Kiyotaka Ijichi', 'Maki Zenin', 'Yuji Itadori', 'Noritosho Kamo', 'Nobara Kugisaki', 'Momo Nishimiya', '.DS_Store', 'Curse users', 'Mai Zenin', 'Panda', 'Toge Inumaki', 'Mahito', 'Satoru Gojo', 'Junpei Yoshino', 'Sorcerer', 'Sukuna', 'Civilians', 'Curse', 'Megumi Fushiguro', 'Aoi Todo', 'Kasumi Miwa']\n",
      "Number of Kento Nanami training char files is 126\n",
      "Number of Kiyotaka Ijichi training char files is 56\n",
      "Number of Maki Zenin training char files is 141\n",
      "Number of Yuji Itadori training char files is 760\n",
      "Number of Noritosho Kamo training char files is 60\n",
      "Number of Nobara Kugisaki training char files is 308\n",
      "Number of Momo Nishimiya training char files is 60\n",
      "Number of Curse users training char files is 41\n",
      "Number of Mai Zenin training char files is 80\n",
      "Number of Panda training char files is 21\n",
      "Number of Toge Inumaki training char files is 57\n",
      "Number of Mahito training char files is 135\n",
      "Number of Satoru Gojo training char files is 222\n",
      "Number of Junpei Yoshino training char files is 110\n",
      "Number of Sorcerer training char files is 139\n",
      "Number of Sukuna training char files is 86\n",
      "Number of Civilians training char files is 302\n",
      "Number of Curse training char files is 124\n",
      "Number of Megumi Fushiguro training char files is 403\n",
      "Number of Aoi Todo training char files is 173\n",
      "Number of Kasumi Miwa training char files is 66\n"
     ]
    }
   ],
   "source": [
    "char_chosen = \"Yuji Itadori\"\n",
    "char_backup_train_files = os.path.join(backup_train_data, 'Yuji Itadori')\n",
    "print(f'Number of Itadori backup training files is {len(os.listdir(char_backup_train_files))}')\n",
    "char_train_files = os.path.join(TRAINING_DIR, 'Yuji Itadori')\n",
    "print(f'Number of Itadori training  files is {len(os.listdir(char_train_files))}')\n",
    "\n",
    "print(os.listdir(TRAINING_DIR))\n",
    "for dir in os.listdir(TRAINING_DIR):\n",
    "    if not dir.startswith('.'):\n",
    "        char_chosen = f\"{dir}\"\n",
    "        char_train_files = os.path.join(TRAINING_DIR, char_chosen)\n",
    "        print(f'Number of {char_chosen} training char files is {len(os.listdir(char_train_files))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3459 images belonging to 21 classes.\n",
      "Found 862 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch = 42\n",
    "training_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(150,150),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "len = 21\n",
      "Class names: ['Aoi Todo', 'Civilians', 'Curse', 'Curse users', 'Junpei Yoshino', 'Kasumi Miwa', 'Kento Nanami', 'Kiyotaka Ijichi', 'Mahito', 'Mai Zenin', 'Maki Zenin', 'Megumi Fushiguro', 'Momo Nishimiya', 'Nobara Kugisaki', 'Noritosho Kamo', 'Panda', 'Satoru Gojo', 'Sorcerer', 'Sukuna', 'Toge Inumaki', 'Yuji Itadori']\n",
      "train\n",
      "len = 21\n",
      "Class names: ['Aoi Todo', 'Civilians', 'Curse', 'Curse users', 'Junpei Yoshino', 'Kasumi Miwa', 'Kento Nanami', 'Kiyotaka Ijichi', 'Mahito', 'Mai Zenin', 'Maki Zenin', 'Megumi Fushiguro', 'Momo Nishimiya', 'Nobara Kugisaki', 'Noritosho Kamo', 'Panda', 'Satoru Gojo', 'Sorcerer', 'Sukuna', 'Toge Inumaki', 'Yuji Itadori']\n"
     ]
    }
   ],
   "source": [
    "print(f\"val\")\n",
    "class_len = len(validation_generator.class_indices.keys())  # Assuming subdirectories correspond to classes\n",
    "class_names = list(validation_generator.class_indices.keys())  # Assuming subdirectories correspond to classes\n",
    "print(f\"len = {class_len}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"train\")\n",
    "class_len = len(train_generator.class_indices.keys())  # Assuming subdirectories correspond to classes\n",
    "class_names = list(train_generator.class_indices.keys())  # Assuming subdirectories correspond to classes\n",
    "print(f\"len = {class_len}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 5, 5, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 2048)              8192      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " final_output (Dense)        (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27279253 (104.06 MB)\n",
      "Trainable params: 27222037 (103.84 MB)\n",
      "Non-trainable params: 57216 (223.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Load the pretrained model with its weights\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    input_shape=(150, 150, 3),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False\n",
    ")\n",
    "# Add the pretrained MobileNet\n",
    "model.add(base_model)\n",
    "\n",
    "# Features detector\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Dense(units=1024, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=1024, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=512, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Final output : probabilities\n",
    "model.add(layers.Dense(21, activation=\"softmax\", name=\"final_output\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()  # Print the model summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/mx/d86l02y96_744bc55lr4kb880000gn/T/ipykernel_14828/4141438796.py\", line 10, in <module>\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [42,21] and labels shape [882]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_136358]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m      3\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/whybless/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/mx/d86l02y96_744bc55lr4kb880000gn/T/ipykernel_14828/4141438796.py\", line 10, in <module>\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n\nlogits and labels must have the same first dimension, got logits shape [42,21] and labels shape [882]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_136358]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Early stopping\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    patience=15, \n",
    "    mode =\"max\", \n",
    "    verbose=2, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    verbose=2,\n",
    "        z            validation_data=validation_generator,\n",
    "                    callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.show()\n",
    "print(\"\")\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save the model\n",
    "# Save your model in a variable\n",
    "model = model.save(os.path.join(base_path, 'jjk_classifier.h5'))\n",
    "\n",
    "# Inspect parameters\n",
    "total_params = model.count_params()\n",
    "num_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"There are {total_params:,} total parameters in this model.\")\n",
    "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model(\"path/to/your/model.h5\")\n",
    "\n",
    "# User input for image path\n",
    "image_path = input(\"Enter the path to your image: \")\n",
    "\n",
    "# Load and preprocess image\n",
    "img = load_img(image_path, target_size=(150, 150))\n",
    "x = img_to_array(img)\n",
    "# ... Apply further preprocessing steps (normalization, etc.)\n",
    "\n",
    "# Expand dimension and predict\n",
    "x = np.expand_dims(x, axis=0)\n",
    "classes = model.predict(x, batch_size=1)\n",
    "\n",
    "# Print results\n",
    "print(image_path)\n",
    "print(classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
